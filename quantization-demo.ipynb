{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization Demo Notebook"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What version of Python do you have?\n",
        "import sys\n",
        "import platform\n",
        "import torch\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "\n",
        "has_gpu = torch.cuda.is_available()\n",
        "has_mps = torch.backends.mps.is_built()\n",
        "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Python Platform: {platform.platform()}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print()\n",
        "print(f\"Python {sys.version}\")\n",
        "print(f\"Pandas {pd.__version__}\")\n",
        "print(f\"Scikit-Learn {sk.__version__}\")\n",
        "print(\"NVIDIA/CUDA GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
        "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
        "print(f\"Target device is {device}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1717278760842
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from dotenv import dotenv_values\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
        "from pympler import asizeof\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForLanguageModeling, GPTQConfig, TrainingArguments, Trainer, QuantoConfig\n",
        "\n",
        "secrets = dotenv_values(\".env\")\n",
        "HUGGINGFACE_TOKEN = secrets['HUGGINGFACE_TOKEN']\n",
        "HfFolder.save_token(HUGGINGFACE_TOKEN)\n",
        "print(\"saved\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717278782954
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing Memory Requirements and Inference Quality \n",
        "### For Various Quantized Models"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "test_question = \"[INST] How much wood could a woodchuck chuck if a woodchuck could chuck wood? [/INST]\"\n",
        "\n",
        "stackoverflow_question = \"\"\"[INST]\n",
        "Consider the following stackoverflow question:\n",
        "\n",
        "Title: Java: Repeat Task Every Random Seconds\n",
        "\n",
        "Body: <p>I'm already familiar with repeating tasks every n seconds by using Java.util.Timer and Java.util.TimerTask. \n",
        "But lets say I want to print \"Hello World\" to the console every random seconds from 1-5. \n",
        "Unfortunately I'm in a bit of a rush and don't have any code to show so far. Any help would be apriciated.  </p>\n",
        "\n",
        "Tags: <java><repeat>\n",
        "\n",
        "Choose between one of these three tags: HQ, LQ_EDIT, and LQ_CLOSE.\n",
        "\n",
        "HQ: High-quality posts without a single edit.\n",
        "LQ_EDIT: Low-quality posts with a negative score, and multiple community edits. However, they remain open.\n",
        "LQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\n",
        "\n",
        "Only respond with either HQ, LQ_EDIT, or LQ_CLOSE.\n",
        "[/INST]\n",
        "\"\"\"\n",
        "label = \"LQ_EDIT\"\n",
        "\n",
        "def print_size_of_model(_model, name):\n",
        "    torch.save(_model.state_dict(), \"temp.p\")\n",
        "    print(f'{name} size (GB): {round(os.path.getsize(\"temp.p\")/1e9, 2)}')\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def check_prediction(_model, _device, name):\n",
        "    inputs = tokenizer(stackoverflow_question, return_tensors=\"pt\").to(_device)\n",
        "    start_time = time.time()\n",
        "    outputs = model.generate(**inputs, max_new_tokens=6)\n",
        "    end_time = time.time()\n",
        "    text_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"{name} raw text response: {text_response[len(stackoverflow_question):]}\\n\")\n",
        "    \n",
        "    runtime = end_time - start_time\n",
        "    if label in text_response[len(stackoverflow_question):]:\n",
        "        print(f\"{label} ✅ \\n\")\n",
        "    else:\n",
        "        print(f\"{label} ❌ \\n\")\n",
        "    \n",
        "    # print(f\"{name} prediction runtime on {_device}: {runtime:.2f} seconds\")\n",
        "\n",
        "\n",
        "# See model card: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 \n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717278862675
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral-7B-Instruct float32 non-quantized"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, device_map=\"cpu\")\n",
        "print(\"============ Mistral-7B-Instruct float32 non-quantized ==================\")\n",
        "print_size_of_model(model, \"Mistral-7B-Instruct float32 non-quantized\")\n",
        "check_prediction(model, \"cpu\", \"Mistral-7B-Instruct float32 non-quantized\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717279149884
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral-7B-Instruct float16 non-quantized"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"cuda:0\")\n",
        "print(\"============ Mistral-7B-Instruct float16 non-quantized ==================\")\n",
        "print_size_of_model(model, \"Mistral-7B-Instruct float16 non-quantized\")\n",
        "check_prediction(model, \"cuda:0\", \"Mistral-7B-Instruct float16 non-quantized\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717279351841
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral-7B-Instruct float16 8bit quantized"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = None # ensure memory isn't dedicated to a prior instance\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "nf8_config = BitsAndBytesConfig(\n",
        "   load_in_8bit=True,\n",
        "   bnb_8bit_quant_type=\"nf8\",\n",
        "   bnb_8bit_use_double_quant=True,\n",
        "   bnb_8bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, \n",
        "    torch_dtype=torch.float16, \n",
        "    quantization_config=nf8_config,\n",
        "    device_map=\"cuda:0\"\n",
        ")\n",
        "print(\"============ Mistral-7B-Instruct float16 8bit quantized ==================\")\n",
        "print_size_of_model(model, \"Mistral-7B-Instruct float16 8bit quantized\")\n",
        "check_prediction(model, \"cuda:0\", \"Mistral-7B-Instruct float16 8bit quantized\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717279414342
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral-7B-Instruct float16 4bit quantized"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   # Slows down inference at the price of being more \n",
        "   # memory efficient since the linear layers will be quantized twice.\n",
        "   bnb_4bit_use_double_quant=False,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, \n",
        "    torch_dtype=torch.float16, \n",
        "    quantization_config=nf4_config,\n",
        "    device_map=\"cuda:0\"\n",
        ")\n",
        "print(\"============ Mistral-7B-Instruct float16 4bit quantized ==================\")\n",
        "print_size_of_model(model, \"Mistral-7B-Instruct float16 4bit quantized\")\n",
        "check_prediction(model, \"cuda:0\", \"Mistral-7B-Instruct float16 4bit quantized\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717279453808
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Open-Source Stack Overflow Dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ROWS = 1000\n",
        "stackoverflow_df = pd.read_csv('train.csv')[:MAX_ROWS]\n",
        "stackoverflow_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717279454669
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SEED = 999\n",
        "TEST_SIZE = 0.2\n",
        "VALIDATION_SIZE = 0.25  # This is 0.25 of the 80% after the initial split\n",
        "\n",
        "# Stack Overflow prompt template\n",
        "stackoverflow_prompt_template = \"\"\"[INST]\n",
        "Consider the following stackoverflow question:\n",
        "\n",
        "Title: {title}\n",
        "\n",
        "Body: {body}\n",
        "\n",
        "Tags: {tags}\n",
        "\n",
        "Choose between one of these three tags: HQ, LQ_EDIT, and LQ_CLOSE.\n",
        "\n",
        "HQ: High-quality posts without a single edit.\n",
        "LQ_EDIT: Low-quality posts with a negative score, and multiple community edits. However, they remain open.\n",
        "LQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\n",
        "\n",
        "Only respond with either HQ, LQ_EDIT, or LQ_CLOSE. [/INST]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "MAX_BODY_CHAR_LEN = 1000\n",
        "\n",
        "# Columns\n",
        "stackoverflow_df['Body_short'] = stackoverflow_df['Body'].str.slice(0, MAX_BODY_CHAR_LEN)\n",
        "\n",
        "def create_mistral_prompt(row):\n",
        "    return stackoverflow_prompt_template.format(\n",
        "        title=row['Title'],\n",
        "        body=row['Body_short'], \n",
        "        tags=row['Tags'], \n",
        "        correct_label=row['Y']\n",
        "    )\n",
        "def create_mistral_training_prompt(row):\n",
        "    return \"<s>\" + stackoverflow_prompt_template.format(\n",
        "        title=row['Title'],\n",
        "        body=row['Body_short'], \n",
        "        tags=row['Tags'], \n",
        "    ) + f\"\\n{row['Y']}</s>\"\n",
        "\n",
        "stackoverflow_df['mistral_prompt'] = stackoverflow_df.apply(create_mistral_prompt, axis=1)\n",
        "stackoverflow_df['mistral_training_prompt'] = stackoverflow_df.apply(create_mistral_training_prompt, axis=1)\n",
        "\n",
        "train_cols = ['mistral_prompt', 'mistral_training_prompt']\n",
        "y_col = ['Y']\n",
        "\n",
        "# Initial split to get test set\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    stackoverflow_df[train_cols], \n",
        "    stackoverflow_df[y_col], \n",
        "    test_size=TEST_SIZE, \n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Further split the training set to get validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, \n",
        "    y_train_full, \n",
        "    test_size=VALIDATION_SIZE, \n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Output shapes to verify the split\n",
        "print(X_train.shape, X_val.shape, X_test.shape)\n",
        "X_train.head(3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717279454744
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assess Performance of Non Fine-Tuned Quantized Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "MAX_TOKEN_LEN = 1024\n",
        "\n",
        "# Prepare the evaluation function\n",
        "def evaluate_model(model, tokenizer, _X_test, _y_test):\n",
        "    print(f\"evaluating {len(_X_test)} rows of test data\")\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    num_unknowns = 0\n",
        "\n",
        "    for i, row in _X_test.iterrows():\n",
        "        inputs = tokenizer(\n",
        "            row['mistral_prompt'], \n",
        "            return_tensors=\"pt\", \n",
        "            truncation=True, \n",
        "            padding=True, \n",
        "            max_length=MAX_TOKEN_LEN\n",
        "        ).to(\"cuda:0\")\n",
        "\n",
        "        outputs = model.generate(**inputs, max_new_tokens=6)\n",
        "        text_response = tokenizer.decode(\n",
        "            outputs[0], \n",
        "            skip_special_tokens=True,\n",
        "        )[len(row['mistral_prompt']):].strip()    \n",
        "        if \"HQ\" in text_response:\n",
        "            predictions.append(\"HQ\")\n",
        "        elif \"LQ_EDIT\" in text_response:\n",
        "            predictions.append(\"LQ_EDIT\")\n",
        "        elif \"LQ_CLOSE\" in text_response:\n",
        "            predictions.append(\"LQ_CLOSE\")\n",
        "        else:\n",
        "            title_50 = row['mistral_prompt'][55:105]\n",
        "            print(f\"WARNING: unknown found for {title_50}\")\n",
        "            predictions.append(\"UNKNOWN\")\n",
        "            num_unknowns += 1\n",
        "\n",
        "        true_labels.append(_y_test.loc[row.name, 'Y'])\n",
        "\n",
        "    print(f\"Found {num_unknowns} unknown labels\")\n",
        "    \n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(true_labels, predictions, average='macro', zero_division=0)\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(true_labels, predictions, average='micro', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_micro': precision_micro,\n",
        "        'recall_micro': recall_micro,\n",
        "        'f1_micro': f1_micro\n",
        "    }\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = evaluate_model(\n",
        "    model, \n",
        "    tokenizer, \n",
        "    X_test, \n",
        "    y_test\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717280794211
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Metrics Over Test Set\n",
        "\n",
        "##### **Accuracy:** the ratio of correctly predicted observations to the total observations, representing the overall effectiveness of a classification model across multiple classes by measuring the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n",
        "\n",
        "##### **Precision Macro:** the average precision (the ratio of correctly predicted positive observations to all predicted positives) across all classes, ensuring each class is given equal importance regardless of its size or frequency in the data.\n",
        "\n",
        "##### **Recall Macro:** the average recall (the ratio of correctly predicted positive observations to all actual positives) across all classes, treating each class equally.\n",
        "\n",
        "##### **F1 Score Macro:** the harmonic mean of precision and recall for each class independently, averaging these scores ensuring that each class contributes equally to the overall metric.\n",
        "\n",
        "##### **F1 Score Micro:** aggregates the contributions of all classes to compute the overall precision and recall, and then calculating their harmonic mean, effectively giving equal weight to each individual instance rather than each class.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "# \n",
        "print(f\"Precision Macro: {metrics['precision_macro']:.4f}\")\n",
        "# the average recall (the ratio of correctly predicted positive observations to all actual positives) \n",
        "# across all classes, treating each class equally regardless of its frequency in the dataset\n",
        "print(f\"Recall Macro: {metrics['recall_macro']:.4f}\")\n",
        "print(f\"F1 Score Macro: {metrics['f1_macro']:.4f}\")\n",
        "print(f\"F1 Score Micro: {metrics['f1_micro']:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717280019923
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}