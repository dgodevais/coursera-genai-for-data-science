{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QLoRA & Quantization Fine Tuning Demo Notebook"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from dotenv import dotenv_values\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
        "from pympler import asizeof\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForLanguageModeling, GPTQConfig, TrainingArguments, Trainer, QuantoConfig\n",
        "\n",
        "secrets = dotenv_values(\".env\")\n",
        "HUGGINGFACE_TOKEN = secrets['HUGGINGFACE_TOKEN']\n",
        "HfFolder.save_token(HUGGINGFACE_TOKEN)\n",
        "print(\"saved\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1717283831665
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load training data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ROWS = 1000\n",
        "stackoverflow_df = pd.read_csv('train.csv')[:MAX_ROWS]\n",
        "stackoverflow_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717283853656
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SEED = 999\n",
        "TEST_SIZE = 0.2\n",
        "VALIDATION_SIZE = 0.25  # This is 0.25 of the 80% after the initial split\n",
        "\n",
        "# Stack Overflow prompt template\n",
        "stackoverflow_prompt_template = \"\"\"[INST]\n",
        "Consider the following stackoverflow question:\n",
        "\n",
        "Title: {title}\n",
        "\n",
        "Body: {body}\n",
        "\n",
        "Tags: {tags}\n",
        "\n",
        "Choose between one of these three tags: HQ, LQ_EDIT, and LQ_CLOSE.\n",
        "\n",
        "HQ: High-quality posts without a single edit.\n",
        "LQ_EDIT: Low-quality posts with a negative score, and multiple community edits. However, they remain open.\n",
        "LQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\n",
        "\n",
        "Only respond with either HQ, LQ_EDIT, or LQ_CLOSE. [/INST]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "MAX_BODY_CHAR_LEN = 1000\n",
        "\n",
        "# Columns\n",
        "stackoverflow_df['Body_short'] = stackoverflow_df['Body'].str.slice(0, MAX_BODY_CHAR_LEN)\n",
        "\n",
        "def create_mistral_prompt(row):\n",
        "    return stackoverflow_prompt_template.format(\n",
        "        title=row['Title'],\n",
        "        body=row['Body_short'], \n",
        "        tags=row['Tags'], \n",
        "        correct_label=row['Y']\n",
        "    )\n",
        "def create_mistral_training_prompt(row):\n",
        "    return \"<s>\" + stackoverflow_prompt_template.format(\n",
        "        title=row['Title'],\n",
        "        body=row['Body_short'], \n",
        "        tags=row['Tags'], \n",
        "    ) + f\"\\n{row['Y']}</s>\"\n",
        "\n",
        "stackoverflow_df['mistral_prompt'] = stackoverflow_df.apply(create_mistral_prompt, axis=1)\n",
        "stackoverflow_df['mistral_training_prompt'] = stackoverflow_df.apply(create_mistral_training_prompt, axis=1)\n",
        "\n",
        "train_cols = ['mistral_prompt', 'mistral_training_prompt']\n",
        "y_col = ['Y']\n",
        "\n",
        "# Initial split to get test set\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    stackoverflow_df[train_cols], \n",
        "    stackoverflow_df[y_col], \n",
        "    test_size=TEST_SIZE, \n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Further split the training set to get validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, \n",
        "    y_train_full, \n",
        "    test_size=VALIDATION_SIZE, \n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Output shapes to verify the split\n",
        "print(X_train.shape, X_val.shape, X_test.shape)\n",
        "stackoverflow_df.head(3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717283853738
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare quantized model for fine tuning process"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=False,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, \n",
        "    torch_dtype=torch.float16, \n",
        "    quantization_config=nf4_config,\n",
        "    device_map=\"cuda:0\"\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "# enable gradient check pointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# enable quantized training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.5,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# LoRA trainable version of model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# trainable parameter count\n",
        "model.print_trainable_parameters()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717283875840
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize training data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKEN_LEN = 512\n",
        "\n",
        "def split_dataframe(df, train_size=0.6, test_size=0.2, random_state=None):\n",
        "    train_df, remaining_df = train_test_split(\n",
        "        df, \n",
        "        train_size=train_size, \n",
        "        random_state=random_state\n",
        "    )\n",
        "    val_test_ratio = test_size / (1 - train_size)\n",
        "    val_df, test_df = train_test_split(\n",
        "        remaining_df, \n",
        "        test_size=val_test_ratio, \n",
        "        random_state=random_state\n",
        "    )\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "train_df, val_df, test_df = split_dataframe(\n",
        "    stackoverflow_df, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_df['mistral_training_prompt'].head(200).to_csv('train_set.csv', index=False)\n",
        "val_df['mistral_training_prompt'].head(200).to_csv('val_set.csv', index=False)\n",
        "test_df['mistral_training_prompt'].head(200).to_csv('test_set.csv', index=False)\n",
        "\n",
        "dataset = None\n",
        "dataset = load_dataset('csv', data_files={'train': \"train_set.csv\",'validation': \"val_set.csv\"})\n",
        "\n",
        "print(f\"Size of training dataset: {len(dataset['train'])}\")\n",
        "print(f\"Size of validation dataset: {len(dataset['validation'])}\")\n",
        "\n",
        "def generate_and_tokenize_prompt(examples):\n",
        "    result = tokenizer(\n",
        "        examples[\"mistral_training_prompt\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_TOKEN_LEN,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    return result\n",
        "\n",
        "tokenized_data = dataset.map(generate_and_tokenize_prompt, batched=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717283876558
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tune The Mistral Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 4\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 0.02\n",
        "WARMUP_STEPS = 2\n",
        "\n",
        "# Prevent any existing instance from conflicting\n",
        "training_args = None\n",
        "trainer = None\n",
        "\n",
        "# define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    warmup_steps=WARMUP_STEPS\n",
        ")\n",
        "# configure trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"validation\"],\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# train model\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "\n",
        "# renable warnings\n",
        "model.config.use_cache = True\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717283939886
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the performance on the holdout test set"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Prepare the evaluation function\n",
        "def evaluate_model(model, tokenizer, _X_test, _y_test):\n",
        "    print(f\"evaluating {len(_X_test)} rows of test data\")\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for i, row in _X_test.iterrows():\n",
        "        inputs = tokenizer(\n",
        "            row['mistral_prompt'], \n",
        "            return_tensors=\"pt\", \n",
        "            truncation=True, \n",
        "            padding=True, \n",
        "            max_length=MAX_TOKEN_LEN\n",
        "        ).to(\"cuda:0\")\n",
        "\n",
        "        outputs = model.generate(**inputs, max_new_tokens=6)\n",
        "        text_response = tokenizer.decode(\n",
        "            outputs[0], \n",
        "            skip_special_tokens=True,\n",
        "        )[len(row['mistral_prompt']):].strip()    \n",
        "        if \"HQ\" in text_response:\n",
        "            predictions.append(\"HQ\")\n",
        "        elif \"LQ_EDIT\" in text_response:\n",
        "            predictions.append(\"LQ_EDIT\")\n",
        "        elif \"LQ_CLOSE\" in text_response:\n",
        "            predictions.append(\"LQ_CLOSE\")\n",
        "        else:\n",
        "            title_50 = row['mistral_prompt'][55:105]\n",
        "            print(f\"WARNING: unknown found for {title_50}\")\n",
        "            predictions.append(\"UNKNOWN\")\n",
        "\n",
        "        true_labels.append(_y_test.loc[row.name, 'Y'])\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(true_labels, predictions, average='macro', zero_division=0)\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(true_labels, predictions, average='micro', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_micro': precision_micro,\n",
        "        'recall_micro': recall_micro,\n",
        "        'f1_micro': f1_micro\n",
        "    }\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = evaluate_model(\n",
        "    model, \n",
        "    tokenizer, \n",
        "    X_test, \n",
        "    y_test\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision Macro: {metrics['precision_macro']:.4f}\")\n",
        "print(f\"Recall Macro: {metrics['recall_macro']:.4f}\")\n",
        "print(f\"F1 Score Macro: {metrics['f1_macro']:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717259659239
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Metrics Over Test Set\n",
        "\n",
        "##### **Accuracy:** the ratio of correctly predicted observations to the total observations, representing the overall effectiveness of a classification model across multiple classes by measuring the proportion of true results (both true positives and true negatives) among the total number of cases examined.\n",
        "\n",
        "##### **Precision Macro:** the average precision (the ratio of correctly predicted positive observations to all predicted positives) across all classes, ensuring each class is given equal importance regardless of its size or frequency in the data.\n",
        "\n",
        "##### **Recall Macro:** the average recall (the ratio of correctly predicted positive observations to all actual positives) across all classes, treating each class equally.\n",
        "\n",
        "##### **F1 Score Macro:** the harmonic mean of precision and recall for each class independently, averaging these scores ensuring that each class contributes equally to the overall metric.\n",
        "\n",
        "##### **F1 Score Micro:** aggregates the contributions of all classes to compute the overall precision and recall, and then calculating their harmonic mean, effectively giving equal weight to each individual instance rather than each class.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision Macro: {metrics['precision_macro']:.4f}\")\n",
        "print(f\"Recall Macro: {metrics['recall_macro']:.4f}\")\n",
        "print(f\"F1 Score Macro: {metrics['f1_macro']:.4f}\")\n",
        "print(f\"F1 Score Micro: {metrics['f1_micro']:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717282721295
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}